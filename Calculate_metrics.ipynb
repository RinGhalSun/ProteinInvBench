{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "dataset = \"CATH4.3\"\n",
    "method = \"AlphaDesign\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_nll_flatten(S, log_probs):\n",
    "    \"\"\" Negative log probabilities \"\"\"\n",
    "    criterion = torch.nn.NLLLoss(reduction='none')\n",
    "    loss = criterion(log_probs, S)\n",
    "    loss_av = loss.mean()\n",
    "    return loss, loss_av\n",
    "\n",
    "def get_metric(S, log_probs):\n",
    "    nll_loss, _ = loss_nll_flatten(S, log_probs)\n",
    "    chain_mask = torch.ones_like(nll_loss)\n",
    "    loss = torch.sum(nll_loss * chain_mask).cpu().data.numpy()\n",
    "    weight = torch.sum(chain_mask).cpu().data.numpy()\n",
    "    return {\"loss\":loss, \"weight\":weight}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Results on test sets of 'CATH4.2', 'CATH4.3', 'MPNN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_perp_recovery(result):\n",
    "    import torch.nn as nn\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    \n",
    "    all_conf = []\n",
    "    all_rec = []\n",
    "    all_lens = []\n",
    "    for i in range(len(result['title'])):\n",
    "        recovery = (result['true_seq'][i] == result['pred_probs'][i].argmax(dim=1)).float().mean()\n",
    "        \n",
    "        # loss = criterion(torch.log(result['pred_probs'][i]), result['true_seq'][i])\n",
    "        conf = result['pred_probs'][i].max(dim=-1)[0].mean()\n",
    "        \n",
    "\n",
    "        all_conf.append(conf.item())\n",
    "        all_rec.append(recovery.item())\n",
    "        all_lens.append(len(result['true_seq'][i]))\n",
    "    \n",
    "    # print(torch.exp(torch.cat(all_perp).mean()))\n",
    "    all_conf = np.array(all_conf)\n",
    "    all_rec = np.array(all_rec)\n",
    "    all_lens = np.array(all_lens)\n",
    "\n",
    "    summary = {}\n",
    "    if dataset in ['CATH4.2', 'CATH4.3']:\n",
    "        summary['conf(0,100)'] = np.median(all_conf[all_lens<100])\n",
    "        summary['conf(100,300)'] = np.median(all_conf[(100<=all_lens)&(all_lens<300)])\n",
    "        summary['conf(300, 500)'] = np.median(all_conf[(300<=all_lens)&(all_lens<500)])\n",
    "        summary['conffull'] = np.median(all_conf)\n",
    "        \n",
    "        summary['rec(0,100)'] = np.median(all_rec[all_lens<100])\n",
    "        summary['rec(100,300)'] = np.median(all_rec[(100<=all_lens)&(all_lens<300)])\n",
    "        summary['rec(300, 500)'] = np.median(all_rec[(300<=all_lens)&(all_lens<500)])\n",
    "        summary['recfull'] = np.median(all_rec)\n",
    "        \n",
    "    if dataset in ['MPNN']:\n",
    "        summary['conf(0,100)'] = np.median(all_conf[all_lens<100])\n",
    "        summary['conf(100,500)'] = np.median(all_conf[(100<=all_lens)&(all_lens<500)])\n",
    "        summary['conf(500, 1000)'] = np.median(all_conf[(500<=all_lens)&(all_lens<1000)])\n",
    "        summary['conffull'] = np.median(all_conf)\n",
    "        \n",
    "        summary['rec(0,100)'] = np.median(all_rec[all_lens<100])\n",
    "        summary['rec(100,500)'] = np.median(all_rec[(100<=all_lens)&(all_lens<500)])\n",
    "        summary['rec(500, 1000)'] = np.median(all_rec[(500<=all_lens)&(all_lens<1000)])\n",
    "        summary['recfull'] = np.median(all_rec)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers.models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresults.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/OpenCPD/lib/python3.12/site-packages/torch/serialization.py:1026\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1024\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1025\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1026\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1032\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1033\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1034\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/OpenCPD/lib/python3.12/site-packages/torch/serialization.py:1438\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1436\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1437\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1438\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1440\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1441\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1443\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/OpenCPD/lib/python3.12/site-packages/torch/serialization.py:1431\u001b[0m, in \u001b[0;36m_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m   1429\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1430\u001b[0m mod_name \u001b[38;5;241m=\u001b[39m load_module_mapping\u001b[38;5;241m.\u001b[39mget(mod_name, mod_name)\n\u001b[0;32m-> 1431\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmod_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.models'"
     ]
    }
   ],
   "source": [
    "result = torch.load(f\"results.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/results/model_zoom/CATH4.2/ProteinMPNN/results.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCATH4.2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCATH4.3\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMPNN\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProteinMPNN\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPiFold\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m----> 5\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/results/model_zoom/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdataset\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmethod\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/results.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m         summary \u001b[38;5;241m=\u001b[39m summary_perp_recovery(result)\n\u001b[1;32m      7\u001b[0m         summary_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{:.2f}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(summary\u001b[38;5;241m.\u001b[39mvalues()))\n",
      "File \u001b[0;32m~/miniconda3/envs/OpenCPD/lib/python3.12/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/OpenCPD/lib/python3.12/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/OpenCPD/lib/python3.12/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/results/model_zoom/CATH4.2/ProteinMPNN/results.pt'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for dataset in ['CATH4.2', 'CATH4.3', 'MPNN']:\n",
    "    for method in ['ProteinMPNN', 'PiFold']:\n",
    "        result = torch.load(f\"/results/model_zoom/{dataset}/{method}/results.pt\")\n",
    "        summary = summary_perp_recovery(result)\n",
    "        summary_string = '\\t'.join('{:.2f}'.format(x) for x in list(summary.values()))\n",
    "        \n",
    "        print(\"data {} method {} \\t result {}\".format(dataset, method, summary_string))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Results on CASP15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summary_perp_recovery(result):\n",
    "    all_conf = []\n",
    "    all_rec = []\n",
    "    all_class = []\n",
    "    for i in range(len(result['title'])):\n",
    "        recovery = (result['true_seq'][i] == result['pred_probs'][i].argmax(dim=1)).float().mean()\n",
    "        conf = result['pred_probs'][i].max(dim=-1)[0].mean()\n",
    "        \n",
    "        all_conf.append(conf.item())\n",
    "        all_rec.append(recovery.item())\n",
    "        all_class.append(result['classification'][i])\n",
    "    \n",
    "    # print(torch.exp(torch.cat(all_perp).mean()))\n",
    "    all_conf = np.array(all_conf)\n",
    "    all_rec = np.array(all_rec)\n",
    "    all_class = np.array(all_class)\n",
    "    # print(set(all_class))\n",
    "    # print(np.sort(all_rec))\n",
    "\n",
    "    summary = {}\n",
    "    mask = np.array([ one in ['FM'] for one in all_class])\n",
    "    summary['FM'] = np.median(all_conf[mask])\n",
    "    mask = np.array([ one not in ['FM'] for one in all_class])\n",
    "    summary['TBM'] = np.median(all_conf[mask])\n",
    "    mask = np.array([ one in ['TBM-easy'] for one in all_class])\n",
    "    summary['TBM-easy'] = np.median(all_conf[mask])\n",
    "    mask = np.array([ one in ['TBM-hard'] for one in all_class])\n",
    "    summary['TBM-hard'] = np.median(all_conf[mask])\n",
    "    summary['Full'] = np.median(all_conf)\n",
    "    \n",
    "    mask = np.array([ one in ['FM'] for one in all_class])\n",
    "    summary['rec FM'] = np.median(all_rec[mask])\n",
    "    mask = np.array([ one not in ['FM/TBM'] for one in all_class])\n",
    "    summary['rec TBM'] = np.median(all_rec[mask])\n",
    "    mask = np.array([ one in ['TBM-easy'] for one in all_class])\n",
    "    summary['rec TBM-easy'] = np.median(all_rec[mask])\n",
    "    mask = np.array([ one in ['TBM-hard'] for one in all_class])\n",
    "    summary['rec TBM-hard'] = np.median(all_rec[mask])\n",
    "    summary['rec Full'] = np.median(all_rec)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = torch.load(f\"/gaozhangyang/experiments/OpenCPD/results/{dataset}/{method}/results_casp15.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "2\n",
      "20\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "print(sum([one =='FM' for one in result['classification']]))\n",
    "print(sum([one =='FM/TBM' for one in result['classification']]))\n",
    "print(sum([one =='TBM-easy' for one in result['classification']]))\n",
    "print(sum([one =='TBM-hard' for one in result['classification']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data CATH4.2 method StructGNN \t result 0.41\t0.46\t0.48\t0.43\t0.45\t0.35\t0.35\t0.38\t0.35\t0.35\n",
      "data CATH4.2 method GraphTrans \t result 0.39\t0.45\t0.46\t0.42\t0.44\t0.33\t0.36\t0.37\t0.36\t0.36\n",
      "data CATH4.2 method GCA \t result 0.48\t0.52\t0.53\t0.48\t0.50\t0.39\t0.40\t0.41\t0.38\t0.40\n",
      "data CATH4.2 method GVP \t result 0.48\t0.49\t0.50\t0.50\t0.49\t0.37\t0.39\t0.42\t0.39\t0.39\n",
      "data CATH4.2 method AlphaDesign \t result 0.44\t0.49\t0.50\t0.46\t0.48\t0.41\t0.42\t0.46\t0.41\t0.42\n",
      "data CATH4.2 method ProteinMPNN \t result 0.49\t0.52\t0.53\t0.51\t0.52\t0.44\t0.44\t0.46\t0.40\t0.44\n",
      "data CATH4.2 method PiFold \t result 0.52\t0.56\t0.59\t0.53\t0.55\t0.47\t0.47\t0.50\t0.47\t0.47\n",
      "data CATH4.2 method KWDesign \t result 0.55\t0.66\t0.70\t0.62\t0.64\t0.49\t0.55\t0.59\t0.55\t0.54\n",
      "data CATH4.3 method StructGNN \t result 0.40\t0.44\t0.45\t0.43\t0.44\t0.35\t0.36\t0.38\t0.37\t0.36\n",
      "data CATH4.3 method GraphTrans \t result 0.39\t0.45\t0.46\t0.43\t0.45\t0.35\t0.35\t0.37\t0.35\t0.35\n",
      "data CATH4.3 method GCA \t result 0.46\t0.49\t0.51\t0.44\t0.48\t0.37\t0.41\t0.43\t0.40\t0.41\n",
      "data CATH4.3 method GVP \t result 0.47\t0.49\t0.50\t0.48\t0.49\t0.37\t0.39\t0.41\t0.38\t0.39\n",
      "data CATH4.3 method AlphaDesign \t result 0.44\t0.50\t0.50\t0.47\t0.48\t0.40\t0.42\t0.44\t0.44\t0.42\n",
      "data CATH4.3 method ProteinMPNN \t result 0.49\t0.52\t0.53\t0.49\t0.52\t0.44\t0.46\t0.48\t0.43\t0.46\n",
      "data CATH4.3 method PiFold \t result 0.54\t0.55\t0.56\t0.51\t0.54\t0.47\t0.49\t0.52\t0.49\t0.49\n",
      "data CATH4.3 method KWDesign \t result 0.59\t0.66\t0.70\t0.63\t0.65\t0.50\t0.57\t0.59\t0.60\t0.56\n",
      "data MPNN method StructGNN \t result 0.46\t0.49\t0.53\t0.47\t0.48\t0.39\t0.41\t0.42\t0.41\t0.41\n",
      "data MPNN method GraphTrans \t result 0.43\t0.49\t0.51\t0.45\t0.48\t0.38\t0.41\t0.44\t0.40\t0.41\n",
      "data MPNN method GCA \t result 0.45\t0.48\t0.49\t0.45\t0.47\t0.40\t0.43\t0.44\t0.43\t0.43\n",
      "data MPNN method GVP \t result 0.51\t0.53\t0.55\t0.53\t0.53\t0.40\t0.43\t0.47\t0.43\t0.43\n",
      "data MPNN method AlphaDesign \t result 0.49\t0.53\t0.54\t0.50\t0.51\t0.43\t0.46\t0.48\t0.46\t0.46\n",
      "data MPNN method ProteinMPNN \t result 0.56\t0.55\t0.58\t0.55\t0.55\t0.52\t0.52\t0.55\t0.51\t0.52\n",
      "data MPNN method PiFold \t result 0.55\t0.57\t0.59\t0.53\t0.57\t0.52\t0.53\t0.53\t0.52\t0.53\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/gaozhangyang/experiments/OpenCPD/results/MPNN/KWDesign/results_casp15.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m dataset \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mCATH4.2\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mCATH4.3\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMPNN\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m      2\u001b[0m     \u001b[39mfor\u001b[39;00m method \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mStructGNN\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mGraphTrans\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mGCA\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mGVP\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mAlphaDesign\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mProteinMPNN\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mPiFold\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mKWDesign\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m----> 3\u001b[0m         result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mload(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m/gaozhangyang/experiments/OpenCPD/results/\u001b[39;49m\u001b[39m{\u001b[39;49;00mdataset\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mmethod\u001b[39m}\u001b[39;49;00m\u001b[39m/results_casp15.pt\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m         summary \u001b[39m=\u001b[39m summary_perp_recovery(result)\n\u001b[1;32m      5\u001b[0m         summary_string \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m{:.2f}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(summary\u001b[39m.\u001b[39mvalues()))\n",
      "File \u001b[0;32m~/anaconda3/envs/foldingdiff/lib/python3.8/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/foldingdiff/lib/python3.8/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    231\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/foldingdiff/lib/python3.8/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/gaozhangyang/experiments/OpenCPD/results/MPNN/KWDesign/results_casp15.pt'"
     ]
    }
   ],
   "source": [
    "for dataset in ['CATH4.2', 'CATH4.3', 'MPNN']:\n",
    "    for method in ['StructGNN', 'GraphTrans', 'GCA', 'GVP', 'AlphaDesign', 'ProteinMPNN', 'PiFold', 'KWDesign']:\n",
    "        result = torch.load(f\"/gaozhangyang/experiments/OpenCPD/results/{dataset}/{method}/results_casp15.pt\")\n",
    "        summary = summary_perp_recovery(result)\n",
    "        summary_string = '\\t'.join('{:.2f}'.format(x) for x in list(summary.values()))\n",
    "        \n",
    "        print(\"data {} method {} \\t result {}\".format(dataset, method, summary_string))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Results on noisy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_perp_recovery(result):\n",
    "    import torch.nn as nn\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    \n",
    "    all_conf = []\n",
    "    all_rec = []\n",
    "    all_lens = []\n",
    "    for i in range(len(result['title'])):\n",
    "        recovery = (result['true_seq'][i] == result['pred_probs'][i].argmax(dim=1)).float().mean()\n",
    "        \n",
    "        # loss = criterion(torch.log(result['pred_probs'][i]), result['true_seq'][i])\n",
    "        conf = result['pred_probs'][i].max(dim=-1)[0].mean()\n",
    "        \n",
    "\n",
    "        all_conf.append(conf.item())\n",
    "        all_rec.append(recovery.item())\n",
    "        all_lens.append(len(result['true_seq'][i]))\n",
    "    \n",
    "    # print(torch.exp(torch.cat(all_perp).mean()))\n",
    "    all_conf = np.array(all_conf)\n",
    "    all_rec = np.array(all_rec)\n",
    "    all_lens = np.array(all_lens)\n",
    "\n",
    "    summary = {}\n",
    "    if dataset in ['CATH4.2', 'CATH4.3']:\n",
    "        summary['conf(0,100)'] = np.median(all_conf[all_lens<100])\n",
    "        summary['conf(100,300)'] = np.median(all_conf[(100<=all_lens)&(all_lens<300)])\n",
    "        summary['conf(300, 500)'] = np.median(all_conf[(300<=all_lens)&(all_lens<500)])\n",
    "        summary['conffull'] = np.median(all_conf)\n",
    "        \n",
    "        summary['rec(0,100)'] = np.median(all_rec[all_lens<100])\n",
    "        summary['rec(100,300)'] = np.median(all_rec[(100<=all_lens)&(all_lens<300)])\n",
    "        summary['rec(300, 500)'] = np.median(all_rec[(300<=all_lens)&(all_lens<500)])\n",
    "        summary['recfull'] = np.median(all_rec)\n",
    "        \n",
    "    if dataset in ['MPNN']:\n",
    "        summary['conf(0,100)'] = np.median(all_conf[all_lens<100])\n",
    "        summary['conf(100,500)'] = np.median(all_conf[(100<=all_lens)&(all_lens<500)])\n",
    "        summary['conf(500, 1000)'] = np.median(all_conf[(500<=all_lens)&(all_lens<1000)])\n",
    "        summary['conffull'] = np.median(all_conf)\n",
    "        \n",
    "        summary['rec(0,100)'] = np.median(all_rec[all_lens<100])\n",
    "        summary['rec(100,500)'] = np.median(all_rec[(100<=all_lens)&(all_lens<500)])\n",
    "        summary['rec(500, 1000)'] = np.median(all_rec[(500<=all_lens)&(all_lens<1000)])\n",
    "        summary['recfull'] = np.median(all_rec)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data CATH4.3 method StructGNN \t result 0.27\t0.26\t0.28\t0.27\t0.19\t0.20\t0.21\t0.20\n",
      "data CATH4.3 method GraphTrans \t result 0.26\t0.26\t0.27\t0.26\t0.19\t0.19\t0.20\t0.20\n",
      "data CATH4.3 method GCA \t result 0.25\t0.25\t0.26\t0.25\t0.19\t0.19\t0.20\t0.19\n",
      "data CATH4.3 method GVP \t result 0.47\t0.53\t0.56\t0.52\t0.32\t0.37\t0.43\t0.38\n",
      "data CATH4.3 method AlphaDesign \t result 0.16\t0.16\t0.15\t0.16\t0.18\t0.18\t0.18\t0.18\n",
      "data CATH4.3 method ProteinMPNN \t result 0.31\t0.30\t0.32\t0.31\t0.22\t0.23\t0.25\t0.23\n",
      "data CATH4.3 method PiFold \t result 0.28\t0.29\t0.32\t0.29\t0.26\t0.28\t0.29\t0.28\n",
      "data CATH4.3 method KWDesign \t result 0.33\t0.42\t0.45\t0.40\t0.29\t0.37\t0.41\t0.35\n"
     ]
    }
   ],
   "source": [
    "dataset = 'CATH4.3'\n",
    "for eps in [0.02, 0.2, 0.5, 1.0]:\n",
    "    for method in ['StructGNN', 'GraphTrans', 'GCA', 'GVP', 'AlphaDesign', 'ProteinMPNN', 'PiFold', 'KWDesign']:\n",
    "        result = torch.load(f\"/gaozhangyang/experiments/OpenCPD/results/{dataset}/{method}_{eps}/results.pt\")\n",
    "        summary = summary_perp_recovery(result)\n",
    "        summary_string = '\\t'.join('{:.2f}'.format(x) for x in list(summary.values()))\n",
    "        \n",
    "        print(\"data {} method {} \\t result {}\".format(dataset, method, summary_string))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "foldingdiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
